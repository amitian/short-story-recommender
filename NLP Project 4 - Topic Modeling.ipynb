{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7561e91e",
   "metadata": {},
   "source": [
    "# NLP Project Pt. 4: Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7284ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>WORDCOUNT</th>\n",
       "      <th>TEXT_NOUNS</th>\n",
       "      <th>TEXT_NOUNS_ADJS</th>\n",
       "      <th>TEXT_NOUNS_VERBS</th>\n",
       "      <th>TEXT_NOUNS_VERBS_ADJS</th>\n",
       "      <th>TEXT_ENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.newyorker.com/magazine/2022/02/14/annunciation</td>\n",
       "      <td>ANNUNCIATION</td>\n",
       "      <td>LAUREN GROFF</td>\n",
       "      <td>February 7, 2022</td>\n",
       "      <td>['Some nights, in my dreams, I find myself running through those hills above Palo Alto again. It is always just before dawn, and as I run I smell the sun-crisped fields, the sage, the eucalyptus. The mist falls in starched sheets over the distant...</td>\n",
       "      <td>9373</td>\n",
       "      <td>night dream hill dawn sun field sage eucalyptus mist sheet hill footstep breath peloton cyclist morning fog neighborhood river asphalt road oak grace block time eye pool house moss bougainvillea fern year life parent college graduation dozen carn...</td>\n",
       "      <td>night dream hill dawn sun field sage eucalyptus mist starched sheet distant hill footstep breath peloton cyclist morning fog quiet wealthy neighborhood black river asphalt road great strong armed oak grace block time eye converted pool house moss...</td>\n",
       "      <td>night dream find run hill dawn run smell sun crisp field sage eucalyptus mist fall sheet hill press hear footstep breath peloton cyclist whir morning fog swallow descend neighborhood river asphalt road flatten fly oak spread grace block time awak...</td>\n",
       "      <td>night dream find run hill dawn run smell sun crisp field sage eucalyptus mist fall starched sheet distant hill press hear footstep breath peloton cyclist whir morning fog swallow descend quiet wealthy neighborhood black river asphalt road flatten...</td>\n",
       "      <td>[Palo Alto, Bay, Mountain View, New England, San Francisco, Chinatown, San Francisco, Redwood City, Mountain View, Titania, Germany, Feuerzangenbowle, Redwood City, Mountain View, New York, Caribbean, the British Virgin Islands, Redwood City, Red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.newyorker.com/magazine/2022/02/07/once-removed</td>\n",
       "      <td>ONCE REMOVED</td>\n",
       "      <td>ALEXANDER MACLEOD</td>\n",
       "      <td>January 31, 2022</td>\n",
       "      <td>['She did not want to visit the old lady.', 'Amy studied the stroller, then the bags, then her boyfriend and the baby. She checked her phone: 11:26a.m. It was time to go. Ninety degrees, ninety-per-cent humidity, and, according to Google, more th...</td>\n",
       "      <td>7778</td>\n",
       "      <td>lady stroller bag boyfriend baby phone time degree cent humidity hour way stage icon event minute end min min bus min walk min effort afternoon abort mission baby phone number furnace shoulder humming trick nap eye breathing rivulet drool spine p...</td>\n",
       "      <td>old lady stroller bag boyfriend baby phone time degree cent humidity hour way stage icon olympic event separate minute end min min bus min walk min worth effort hot afternoon abort mission sorry baby right phone number furnace shoulder humming tr...</td>\n",
       "      <td>want visit lady study stroller bag boyfriend baby check phone time degree cent humidity accord hour way stage icon event minute break total end walk min train min bus min walk min effort afternoon abort mission abort baby  phone number  furnace h...</td>\n",
       "      <td>want visit old lady study stroller bag boyfriend baby check phone time degree cent humidity accord hour way stage icon olympic event separate minute break total end walk min train min bus min walk min worth effort hot afternoon abort mission abor...</td>\n",
       "      <td>[Inverness County, Nova Scotia, Ontario, Toronto, Montreal, Cape Breton, Turkey, Niagara Falls, Montreal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.newyorker.com/magazine/2022/01/31/long-distance</td>\n",
       "      <td>LONG DISTANCE</td>\n",
       "      <td>AYSEGUL SAVAS</td>\n",
       "      <td>January 24, 2022</td>\n",
       "      <td>['Lea changed the sheets when she got up. She’d bought flowers the previous day, tulips that she’d put on the dresser. There were carnations on the kitchen table, in a squat glass vase. She thought they looked cheerful, and not too fussy.', 'The ...</td>\n",
       "      <td>4866</td>\n",
       "      <td>sheet flower day tulip dresser carnation kitchen table glass vase fridge thing olive jam prosciutto cheese wine beer cookie bread cracker café place mind scene bed people mess plate idea indulgence sheet light hour imagination afternoon reason be...</td>\n",
       "      <td>sheet flower previous day tulip dresser carnation kitchen table squat glass vase cheerful fussy fridge thing able olive jam prosciutto cheese wine beer cookie bread round taralli cracker common roman café place mind scene bed people mess plate id...</td>\n",
       "      <td>change sheet buy flower day tulip dresser carnation kitchen table glass vase think look fridge fill thing eat olive jam prosciutto cheese buy wine beer cookie bread cracker café think stay place want mind scene eat bed people mess plate like idea...</td>\n",
       "      <td>change sheet buy flower previous day tulip dresser carnation kitchen table squat glass vase think look cheerful fussy fridge fill thing able eat olive jam prosciutto cheese buy wine beer cookie bread round taralli cracker common roman café think ...</td>\n",
       "      <td>[California, Rome, Rome, Rome, California, Rome, Trastevere, the Ponte Sublicio, Everest, Rome, Rome, Rome, California, Ostiense, Rome, San Pietro, Vincoli, Rome, Rome, London, California]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.newyorker.com/magazine/2022/01/24/whats-the-deal-hummingbird</td>\n",
       "      <td>WHAT'S THE DEAL, HUMMINGBIRD?</td>\n",
       "      <td>ARTHUR KRYSTAL</td>\n",
       "      <td>January 17, 2022</td>\n",
       "      <td>['On or around May 5th of 2020, he just stopped. He stopped exercising, stopped walking, stopped reading, stopped planning. He ate, drank, washed, and paid the bills, but that was it. He was seventy-three. He’d spent more than 38,368,800 minutes ...</td>\n",
       "      <td>3469</td>\n",
       "      <td>5th bill minute earth evening cheering clanking pot pan chunk life life time million minute lunch dinner meeting concert marriage work book movie conversation bird breath existence mother bird dress year world course squirrel bb gun body footrace...</td>\n",
       "      <td>5th bill minute earth precious evening cheering clanking pot pan vast chunk life great life time million minute lunch dinner meeting concert marriage work book movie conversation bird breath existence mother actual bird dress year old world cours...</td>\n",
       "      <td>5th stop stop exercise stop walk stop read stop plan eat drink wash pay bill spend minute earth remember  hit evening cheering clanking pot pan die chunk life life occur time spend sleep million minute span lunch dinner meeting concert marriage w...</td>\n",
       "      <td>5th stop stop exercise stop walk stop read stop plan eat drink wash pay bill spend minute earth precious remember  hit evening cheering clanking pot pan die vast chunk life great life occur time spend sleep million minute span lunch dinner meetin...</td>\n",
       "      <td>[Prospect Park, Kentucky, Tanglewood, East, Provence, Montpellier, Nice, Brooklyn, West Orange, New Jersey, New Orleans, San Francisco, New York, Corfu, America, America, America, America, America, Vietnam, America, America, America, Italy, Uffiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.newyorker.com/magazine/2022/01/17/fireworks</td>\n",
       "      <td>FIREWORKS</td>\n",
       "      <td>GRAHAM SWIFT</td>\n",
       "      <td>January 10, 2022</td>\n",
       "      <td>['It was late October, 1962. Russian missiles were being shipped to Cuba. Kennedy was having words with Khrushchev. The world might be coming to an end.', 'It was a common remark: “Cheer up, it’s not the end of the world.”', 'Frank Greene’s wife,...</td>\n",
       "      <td>2687</td>\n",
       "      <td>missile word world end remark end world wife look fear face world wife end wedding bedroom tear dress week evening people day week week daughter fortnight sum money point day work event end world tone look face news tv wife shoulder hand force st...</td>\n",
       "      <td>late russian missile word world end common remark end world wife look genuine fear face world silly flippant wife distraught end wedding bedroom tear dress week evening people good humored bad day week ordinary week daughter fortnight ready huge ...</td>\n",
       "      <td>missile ship word world come end remark cheer  end world wife look fear face world end know sound wife look come end wedding shut bedroom let  tear collect dress week collect evening people dread day week resign week daughter marry fortnight fork...</td>\n",
       "      <td>late russian missile ship word world come end common remark cheer  end world wife look genuine fear face world end silly know sound flippant wife look distraught come end wedding shut bedroom let  tear collect dress week collect evening people dr...</td>\n",
       "      <td>[Cuba, Harpers]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        URL  \\\n",
       "0                https://www.newyorker.com/magazine/2022/02/14/annunciation   \n",
       "1                https://www.newyorker.com/magazine/2022/02/07/once-removed   \n",
       "2               https://www.newyorker.com/magazine/2022/01/31/long-distance   \n",
       "3  https://www.newyorker.com/magazine/2022/01/24/whats-the-deal-hummingbird   \n",
       "4                   https://www.newyorker.com/magazine/2022/01/17/fireworks   \n",
       "\n",
       "                           TITLE             AUTHOR              DATE  \\\n",
       "0                   ANNUNCIATION       LAUREN GROFF  February 7, 2022   \n",
       "1                   ONCE REMOVED  ALEXANDER MACLEOD  January 31, 2022   \n",
       "2                  LONG DISTANCE      AYSEGUL SAVAS  January 24, 2022   \n",
       "3  WHAT'S THE DEAL, HUMMINGBIRD?     ARTHUR KRYSTAL  January 17, 2022   \n",
       "4                      FIREWORKS       GRAHAM SWIFT  January 10, 2022   \n",
       "\n",
       "                                                                                                                                                                                                                                                        TEXT  \\\n",
       "0  ['Some nights, in my dreams, I find myself running through those hills above Palo Alto again. It is always just before dawn, and as I run I smell the sun-crisped fields, the sage, the eucalyptus. The mist falls in starched sheets over the distant...   \n",
       "1  ['She did not want to visit the old lady.', 'Amy studied the stroller, then the bags, then her boyfriend and the baby. She checked her phone: 11:26a.m. It was time to go. Ninety degrees, ninety-per-cent humidity, and, according to Google, more th...   \n",
       "2  ['Lea changed the sheets when she got up. She’d bought flowers the previous day, tulips that she’d put on the dresser. There were carnations on the kitchen table, in a squat glass vase. She thought they looked cheerful, and not too fussy.', 'The ...   \n",
       "3  ['On or around May 5th of 2020, he just stopped. He stopped exercising, stopped walking, stopped reading, stopped planning. He ate, drank, washed, and paid the bills, but that was it. He was seventy-three. He’d spent more than 38,368,800 minutes ...   \n",
       "4  ['It was late October, 1962. Russian missiles were being shipped to Cuba. Kennedy was having words with Khrushchev. The world might be coming to an end.', 'It was a common remark: “Cheer up, it’s not the end of the world.”', 'Frank Greene’s wife,...   \n",
       "\n",
       "   WORDCOUNT  \\\n",
       "0       9373   \n",
       "1       7778   \n",
       "2       4866   \n",
       "3       3469   \n",
       "4       2687   \n",
       "\n",
       "                                                                                                                                                                                                                                                  TEXT_NOUNS  \\\n",
       "0  night dream hill dawn sun field sage eucalyptus mist sheet hill footstep breath peloton cyclist morning fog neighborhood river asphalt road oak grace block time eye pool house moss bougainvillea fern year life parent college graduation dozen carn...   \n",
       "1  lady stroller bag boyfriend baby phone time degree cent humidity hour way stage icon event minute end min min bus min walk min effort afternoon abort mission baby phone number furnace shoulder humming trick nap eye breathing rivulet drool spine p...   \n",
       "2  sheet flower day tulip dresser carnation kitchen table glass vase fridge thing olive jam prosciutto cheese wine beer cookie bread cracker café place mind scene bed people mess plate idea indulgence sheet light hour imagination afternoon reason be...   \n",
       "3  5th bill minute earth evening cheering clanking pot pan chunk life life time million minute lunch dinner meeting concert marriage work book movie conversation bird breath existence mother bird dress year world course squirrel bb gun body footrace...   \n",
       "4  missile word world end remark end world wife look fear face world wife end wedding bedroom tear dress week evening people day week week daughter fortnight sum money point day work event end world tone look face news tv wife shoulder hand force st...   \n",
       "\n",
       "                                                                                                                                                                                                                                             TEXT_NOUNS_ADJS  \\\n",
       "0  night dream hill dawn sun field sage eucalyptus mist starched sheet distant hill footstep breath peloton cyclist morning fog quiet wealthy neighborhood black river asphalt road great strong armed oak grace block time eye converted pool house moss...   \n",
       "1  old lady stroller bag boyfriend baby phone time degree cent humidity hour way stage icon olympic event separate minute end min min bus min walk min worth effort hot afternoon abort mission sorry baby right phone number furnace shoulder humming tr...   \n",
       "2  sheet flower previous day tulip dresser carnation kitchen table squat glass vase cheerful fussy fridge thing able olive jam prosciutto cheese wine beer cookie bread round taralli cracker common roman café place mind scene bed people mess plate id...   \n",
       "3  5th bill minute earth precious evening cheering clanking pot pan vast chunk life great life time million minute lunch dinner meeting concert marriage work book movie conversation bird breath existence mother actual bird dress year old world cours...   \n",
       "4  late russian missile word world end common remark end world wife look genuine fear face world silly flippant wife distraught end wedding bedroom tear dress week evening people good humored bad day week ordinary week daughter fortnight ready huge ...   \n",
       "\n",
       "                                                                                                                                                                                                                                            TEXT_NOUNS_VERBS  \\\n",
       "0  night dream find run hill dawn run smell sun crisp field sage eucalyptus mist fall sheet hill press hear footstep breath peloton cyclist whir morning fog swallow descend neighborhood river asphalt road flatten fly oak spread grace block time awak...   \n",
       "1  want visit lady study stroller bag boyfriend baby check phone time degree cent humidity accord hour way stage icon event minute break total end walk min train min bus min walk min effort afternoon abort mission abort baby  phone number  furnace h...   \n",
       "2  change sheet buy flower day tulip dresser carnation kitchen table glass vase think look fridge fill thing eat olive jam prosciutto cheese buy wine beer cookie bread cracker café think stay place want mind scene eat bed people mess plate like idea...   \n",
       "3  5th stop stop exercise stop walk stop read stop plan eat drink wash pay bill spend minute earth remember  hit evening cheering clanking pot pan die chunk life life occur time spend sleep million minute span lunch dinner meeting concert marriage w...   \n",
       "4  missile ship word world come end remark cheer  end world wife look fear face world end know sound wife look come end wedding shut bedroom let  tear collect dress week collect evening people dread day week resign week daughter marry fortnight fork...   \n",
       "\n",
       "                                                                                                                                                                                                                                       TEXT_NOUNS_VERBS_ADJS  \\\n",
       "0  night dream find run hill dawn run smell sun crisp field sage eucalyptus mist fall starched sheet distant hill press hear footstep breath peloton cyclist whir morning fog swallow descend quiet wealthy neighborhood black river asphalt road flatten...   \n",
       "1  want visit old lady study stroller bag boyfriend baby check phone time degree cent humidity accord hour way stage icon olympic event separate minute break total end walk min train min bus min walk min worth effort hot afternoon abort mission abor...   \n",
       "2  change sheet buy flower previous day tulip dresser carnation kitchen table squat glass vase think look cheerful fussy fridge fill thing able eat olive jam prosciutto cheese buy wine beer cookie bread round taralli cracker common roman café think ...   \n",
       "3  5th stop stop exercise stop walk stop read stop plan eat drink wash pay bill spend minute earth precious remember  hit evening cheering clanking pot pan die vast chunk life great life occur time spend sleep million minute span lunch dinner meetin...   \n",
       "4  late russian missile ship word world come end common remark cheer  end world wife look genuine fear face world end silly know sound flippant wife look distraught come end wedding shut bedroom let  tear collect dress week collect evening people dr...   \n",
       "\n",
       "                                                                                                                                                                                                                                                   TEXT_ENTS  \n",
       "0  [Palo Alto, Bay, Mountain View, New England, San Francisco, Chinatown, San Francisco, Redwood City, Mountain View, Titania, Germany, Feuerzangenbowle, Redwood City, Mountain View, New York, Caribbean, the British Virgin Islands, Redwood City, Red...  \n",
       "1                                                                                                                                                  [Inverness County, Nova Scotia, Ontario, Toronto, Montreal, Cape Breton, Turkey, Niagara Falls, Montreal]  \n",
       "2                                                               [California, Rome, Rome, Rome, California, Rome, Trastevere, the Ponte Sublicio, Everest, Rome, Rome, Rome, California, Ostiense, Rome, San Pietro, Vincoli, Rome, Rome, London, California]  \n",
       "3  [Prospect Park, Kentucky, Tanglewood, East, Provence, Montpellier, Nice, Brooklyn, West Orange, New Jersey, New Orleans, San Francisco, New York, Corfu, America, America, America, America, America, Vietnam, America, America, America, Italy, Uffiz...  \n",
       "4                                                                                                                                                                                                                                            [Cuba, Harpers]  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('max_colwidth',250)\n",
    "\n",
    "corpus_df = pd.read_pickle('corpus_df_spacy.pkl')\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268fbe5f",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab202c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "#function from course notebooks\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words, topic_names = None): \n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix + 1)\n",
    "        else:\n",
    "            print(\"\\nTopic: \", topic_names[ix])\n",
    "        print([[feature_names[i], topic[i].round(3)] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "    print(\"\\n\")\n",
    "    return model, feature_names, no_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f7132",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67bb983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ami/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "[['parents', 3.23], ['phone', 3.029], ['town', 2.498], ['son', 2.327], ['city', 2.309], ['road', 2.25], ['story', 2.245], ['husband', 2.17], ['watched', 2.007], ['apartment', 2.005], ['daughter', 1.966], ['boys', 1.956], ['job', 1.87], ['doesn', 1.859], ['baby', 1.835], ['girls', 1.828], ['sleep', 1.816], ['wall', 1.812], ['dinner', 1.787], ['book', 1.786]]\n",
      "\n",
      "Topic  2\n",
      "[['says', 23.031], ['doesn', 2.497], ['asks', 2.18], ['looks', 2.169], ['thinks', 1.516], ['takes', 1.434], ['goes', 1.413], ['comes', 1.236], ['knows', 1.196], ['turns', 1.158], ['feels', 1.072], ['makes', 1.052], ['mark', 0.996], ['gets', 0.952], ['isn', 0.941], ['tells', 0.898], ['husband', 0.854], ['wants', 0.848], ['sees', 0.772], ['guy', 0.751]]\n",
      "\n",
      "Topic  3\n",
      "[['jack', 15.358], ['olive', 3.808], ['dog', 2.704], ['phone', 1.155], ['road', 0.946], ['kids', 0.678], ['corn', 0.633], ['farmer', 0.59], ['mom', 0.584], ['girls', 0.576], ['son', 0.574], ['ain', 0.566], ['yeah', 0.487], ['sky', 0.484], ['shit', 0.484], ['hello', 0.446], ['question', 0.445], ['quickly', 0.442], ['needed', 0.441], ['sun', 0.429]]\n",
      "\n",
      "Topic  4\n",
      "[['mr', 15.523], ['mrs', 5.913], ['feather', 3.147], ['fish', 2.156], ['hat', 1.794], ['dad', 1.645], ['bus', 1.642], ['boys', 1.295], ['line', 0.911], ['box', 0.907], ['grace', 0.874], ['arm', 0.84], ['cap', 0.811], ['office', 0.76], ['lady', 0.748], ['corner', 0.745], ['shop', 0.741], ['ran', 0.709], ['road', 0.686], ['driver', 0.681]]\n",
      "\n",
      "Topic  5\n",
      "[['paul', 13.178], ['crystal', 2.695], ['mom', 1.328], ['dad', 1.293], ['god', 1.1], ['baby', 1.047], ['kids', 0.762], ['fine', 0.722], ['son', 0.646], ['guy', 0.637], ['actually', 0.637], ['nice', 0.579], ['babies', 0.575], ['kid', 0.564], ['loved', 0.557], ['office', 0.531], ['happy', 0.518], ['ma', 0.5], ['sorry', 0.499], ['wait', 0.469]]\n",
      "\n",
      "Topic  6\n",
      "[['sister', 12.457], ['cat', 4.925], ['older', 2.61], ['baby', 2.584], ['uncle', 2.136], ['mouse', 1.936], ['girls', 1.768], ['dog', 1.573], ['brother', 1.468], ['husband', 1.462], ['eat', 1.033], ['god', 0.99], ['mama', 0.963], ['couch', 0.958], ['st', 0.86], ['food', 0.795], ['lady', 0.79], ['sugar', 0.749], ['watched', 0.73], ['yellow', 0.728]]\n",
      "\n",
      "Topic  7\n",
      "[['james', 14.139], ['mrs', 1.055], ['son', 0.861], ['restaurant', 0.81], ['willing', 0.695], ['asks', 0.662], ['mary', 0.637], ['store', 0.587], ['doctor', 0.573], ['baby', 0.549], ['doesn', 0.535], ['young man', 0.521], ['daughter', 0.498], ['thinks', 0.493], ['sleep', 0.49], ['land', 0.459], ['teacher', 0.44], ['country', 0.427], ['english', 0.425], ['feels', 0.422]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NMF with CV on original text\n",
    "docs = corpus_df.TEXT\n",
    "cv = CountVectorizer(stop_words='english', min_df=0.05, max_df=0.6, ngram_range=(1,2))\n",
    "doc_term = cv.fit_transform(docs)\n",
    "nmf = NMF(7)\n",
    "nmf.fit(doc_term)\n",
    "\n",
    "output = display_topics(nmf, cv.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e268bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ami/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "[['husband', 0.341], ['parents', 0.33], ['daughter', 0.243], ['apartment', 0.239], ['story', 0.237], ['phone', 0.203], ['book', 0.196], ['son', 0.18], ['class', 0.174], ['married', 0.173], ['city', 0.166], ['evening', 0.165], ['teacher', 0.163], ['marriage', 0.147], ['books', 0.146], ['dinner', 0.145], ['reading', 0.143], ['train', 0.141], ['spoke', 0.141], ['girls', 0.141]]\n",
      "\n",
      "Topic  2\n",
      "[['says', 1.896], ['asks', 0.357], ['doesn', 0.27], ['looks', 0.244], ['thinks', 0.225], ['takes', 0.191], ['feels', 0.186], ['goes', 0.178], ['tells', 0.162], ['turns', 0.161], ['comes', 0.154], ['knows', 0.148], ['sees', 0.144], ['makes', 0.13], ['sits', 0.126], ['wants', 0.124], ['puts', 0.124], ['gets', 0.117], ['starts', 0.117], ['stands', 0.106]]\n",
      "\n",
      "Topic  3\n",
      "[['james', 1.702], ['doctor', 0.065], ['ship', 0.056], ['daughter', 0.052], ['mary', 0.049], ['son', 0.044], ['motel', 0.039], ['ain', 0.039], ['willing', 0.038], ['dog', 0.038], ['mr', 0.038], ['yeah', 0.037], ['maid', 0.036], ['book', 0.034], ['feels', 0.033], ['deck', 0.033], ['captain', 0.033], ['baby', 0.033], ['storm', 0.033], ['land', 0.032]]\n",
      "\n",
      "Topic  4\n",
      "[['road', 0.26], ['dog', 0.229], ['trees', 0.184], ['river', 0.183], ['wind', 0.17], ['sun', 0.165], ['sky', 0.154], ['rain', 0.154], ['town', 0.147], ['old man', 0.145], ['boys', 0.143], ['rose', 0.137], ['green', 0.135], ['sea', 0.134], ['ran', 0.127], ['grass', 0.127], ['stone', 0.127], ['ground', 0.127], ['tree', 0.119], ['watched', 0.119]]\n",
      "\n",
      "Topic  5\n",
      "[['dad', 0.386], ['mom', 0.35], ['baby', 0.348], ['guy', 0.331], ['kids', 0.306], ['kid', 0.285], ['yeah', 0.24], ['fucking', 0.232], ['brother', 0.215], ['phone', 0.21], ['shit', 0.204], ['fuck', 0.199], ['jack', 0.156], ['guys', 0.152], ['hey', 0.146], ['ass', 0.144], ['sorry', 0.138], ['nice', 0.134], ['somebody', 0.131], ['actually', 0.125]]\n",
      "\n",
      "Topic  6\n",
      "[['mrs', 1.192], ['mr', 0.733], ['dr', 0.104], ['bus', 0.099], ['grace', 0.09], ['girls', 0.087], ['driver', 0.079], ['monkey', 0.079], ['mary', 0.068], ['miss', 0.067], ['hat', 0.063], ['dad', 0.063], ['husband', 0.062], ['stolen', 0.062], ['sir', 0.057], ['town', 0.053], ['grandfather', 0.053], ['birthday', 0.053], ['upstairs', 0.052], ['grandmother', 0.052]]\n",
      "\n",
      "Topic  7\n",
      "[['paul', 1.454], ['baby', 0.131], ['crystal', 0.13], ['aunt', 0.073], ['parents', 0.061], ['boyfriend', 0.059], ['ma', 0.056], ['st', 0.053], ['babies', 0.053], ['daughter', 0.048], ['lighter', 0.048], ['asshole', 0.047], ['letter', 0.046], ['ha', 0.046], ['mary', 0.044], ['magic', 0.043], ['teacher', 0.043], ['bedroom', 0.043], ['god', 0.041], ['lesson', 0.04]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#trying NMF with TF-IDF on original text with bigrams\n",
    "docs = corpus_df.TEXT\n",
    "tf = TfidfVectorizer(stop_words='english', min_df = 0.05, max_df=0.60, ngram_range=(1,2))\n",
    "doc_term = tf.fit_transform(docs)\n",
    "nmf = NMF(7)\n",
    "nmf.fit(doc_term)\n",
    "\n",
    "output = display_topics(nmf, tf.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be0b435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "[['apartment', 0.424], ['student', 0.315], ['class', 0.298], ['train', 0.273], ['office', 0.27], ['hotel', 0.268], ['bar', 0.263], ['painting', 0.255], ['building', 0.241], ['dinner', 0.241], ['writer', 0.238], ['coffee', 0.235], ['restaurant', 0.233], ['party', 0.233], ['city', 0.232], ['drink', 0.22], ['desk', 0.22], ['teacher', 0.22], ['movie', 0.218], ['film', 0.214]]\n",
      "\n",
      "Topic  2\n",
      "[['road', 0.46], ['river', 0.268], ['wind', 0.265], ['stone', 0.264], ['sun', 0.26], ['field', 0.258], ['sea', 0.252], ['horse', 0.24], ['grass', 0.23], ['sky', 0.226], ['wood', 0.215], ['beach', 0.214], ['boat', 0.204], ['rain', 0.199], ['ground', 0.196], ['truck', 0.195], ['lake', 0.193], ['bird', 0.191], ['rock', 0.191], ['mountain', 0.19]]\n",
      "\n",
      "Topic  3\n",
      "[['kid', 0.863], ['guy', 0.818], ['mom', 0.462], ['shit', 0.266], ['dad', 0.265], ['cop', 0.26], ['brother', 0.256], ['ass', 0.225], ['yard', 0.18], ['son', 0.179], ['gun', 0.169], ['movie', 0.161], ['fuck', 0.154], ['stuff', 0.144], ['hell', 0.131], ['tv', 0.13], ['dollar', 0.129], ['dude', 0.123], ['lady', 0.122], ['game', 0.119]]\n",
      "\n",
      "Topic  4\n",
      "[['baby', 1.378], ['nurse', 0.265], ['doctor', 0.256], ['sister', 0.189], ['hospital', 0.16], ['diaper', 0.155], ['stroller', 0.141], ['skin', 0.139], ['belly', 0.121], ['couch', 0.106], ['breast', 0.102], ['milk', 0.1], ['grandmother', 0.091], ['stomach', 0.082], ['bottle', 0.081], ['infant', 0.08], ['blanket', 0.079], ['aunt', 0.079], ['chest', 0.078], ['box', 0.077]]\n",
      "\n",
      "Topic  5\n",
      "[['dog', 1.646], ['doctor', 0.16], ['leash', 0.133], ['porch', 0.129], ['gate', 0.119], ['yard', 0.113], ['animal', 0.103], ['mask', 0.094], ['paw', 0.086], ['tail', 0.085], ['fear', 0.084], ['bird', 0.081], ['bottle', 0.081], ['fur', 0.079], ['blood', 0.077], ['road', 0.077], ['pet', 0.077], ['fence', 0.073], ['killer', 0.072], ['shovel', 0.072]]\n",
      "\n",
      "Topic  6\n",
      "[['husband', 0.866], ['daughter', 0.725], ['son', 0.629], ['brother', 0.345], ['sister', 0.336], ['law', 0.271], ['grandmother', 0.246], ['doctor', 0.228], ['village', 0.223], ['marriage', 0.207], ['cousin', 0.2], ['grandfather', 0.192], ['wedding', 0.176], ['death', 0.175], ['hospital', 0.17], ['church', 0.168], ['uncle', 0.154], ['letter', 0.15], ['neighbor', 0.13], ['tea', 0.12]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#best topics\n",
    "\n",
    "#NMF + TF-IDF + nouns only\n",
    "\n",
    "docs = corpus_df.TEXT_NOUNS\n",
    "tf = TfidfVectorizer(stop_words='english', min_df = 0.03, max_df=0.55)\n",
    "doc_term = tf.fit_transform(docs)\n",
    "nmf = NMF(n_components=6, init='nndsvda', max_iter=450)\n",
    "nmf.fit(doc_term)\n",
    "\n",
    "output = display_topics(nmf, tf.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d74a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "[['parent', 0.381], ['apartment', 0.317], ['student', 0.311], ['husband', 0.308], ['class', 0.274], ['phone', 0.267], ['love', 0.254], ['daughter', 0.25], ['teacher', 0.237], ['office', 0.236], ['painting', 0.218], ['dinner', 0.217], ['city', 0.216], ['evening', 0.215], ['train', 0.21], ['party', 0.209], ['marriage', 0.208], ['college', 0.207], ['letter', 0.204], ['hotel', 0.204]]\n",
      "\n",
      "Topic  2\n",
      "[['road', 0.389], ['wind', 0.244], ['stone', 0.235], ['sun', 0.234], ['river', 0.233], ['field', 0.227], ['town', 0.223], ['village', 0.214], ['sea', 0.21], ['horse', 0.209], ['sky', 0.205], ['grass', 0.2], ['wood', 0.192], ['rain', 0.191], ['beach', 0.179], ['ground', 0.178], ['boat', 0.177], ['green', 0.176], ['truck', 0.171], ['bird', 0.17]]\n",
      "\n",
      "Topic  3\n",
      "[['kid', 0.75], ['guy', 0.743], ['mom', 0.401], ['fucking', 0.274], ['phone', 0.273], ['shit', 0.262], ['brother', 0.248], ['dad', 0.237], ['cop', 0.229], ['ass', 0.22], ['nice', 0.193], ['movie', 0.177], ['fuck', 0.167], ['bag', 0.162], ['stuff', 0.155], ['yard', 0.145], ['bar', 0.143], ['son', 0.143], ['gun', 0.142], ['beer', 0.14]]\n",
      "\n",
      "Topic  4\n",
      "[['baby', 1.059], ['husband', 0.406], ['sister', 0.394], ['daughter', 0.339], ['doctor', 0.307], ['son', 0.273], ['nurse', 0.264], ['grandmother', 0.228], ['hospital', 0.207], ['parent', 0.17], ['law', 0.141], ['brother', 0.134], ['pregnant', 0.119], ['skin', 0.115], ['diaper', 0.112], ['phone', 0.111], ['doll', 0.109], ['blood', 0.107], ['aunt', 0.099], ['bag', 0.099]]\n",
      "\n",
      "Topic  5\n",
      "[['dog', 1.575], ['doctor', 0.146], ['porch', 0.125], ['gate', 0.118], ['yard', 0.106], ['animal', 0.105], ['pet', 0.103], ['mask', 0.088], ['town', 0.084], ['fear', 0.084], ['road', 0.081], ['bird', 0.081], ['paw', 0.081], ['tail', 0.08], ['phone', 0.073], ['fur', 0.071], ['fence', 0.07], ['killer', 0.07], ['blood', 0.069], ['shovel', 0.068]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NMF + TF-IDF + nouns + adjs only\n",
    "docs = corpus_df.TEXT_NOUNS_ADJS\n",
    "tf = TfidfVectorizer(stop_words='english', min_df = 0.05, max_df=0.60)\n",
    "doc_term = tf.fit_transform(docs)\n",
    "nmf = NMF(n_components=5, init='nndsvda', max_iter=450)\n",
    "nmf.fit(doc_term)\n",
    "\n",
    "output = display_topics(nmf, tf.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c5c7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "[['parent', 0.394], ['student', 0.307], ['husband', 0.305], ['apartment', 0.303], ['class', 0.261], ['daughter', 0.26], ['teacher', 0.249], ['office', 0.224], ['marry', 0.216], ['marriage', 0.214], ['letter', 0.213], ['evening', 0.212], ['train', 0.211], ['painting', 0.211], ['city', 0.209], ['college', 0.204], ['dinner', 0.203], ['writer', 0.202], ['restaurant', 0.194], ['party', 0.194]]\n",
      "\n",
      "Topic  2\n",
      "[['road', 0.369], ['wind', 0.24], ['sun', 0.23], ['stone', 0.228], ['river', 0.219], ['sea', 0.21], ['sky', 0.209], ['field', 0.209], ['rain', 0.201], ['town', 0.198], ['rise', 0.19], ['grass', 0.188], ['horse', 0.182], ['wood', 0.179], ['village', 0.177], ['beach', 0.177], ['lift', 0.172], ['boat', 0.171], ['climb', 0.168], ['ground', 0.168]]\n",
      "\n",
      "Topic  3\n",
      "[['kid', 0.742], ['guy', 0.715], ['mom', 0.389], ['fuck', 0.33], ['shit', 0.269], ['brother', 0.265], ['dad', 0.232], ['cop', 0.224], ['ass', 0.213], ['hit', 0.191], ['movie', 0.187], ['kill', 0.165], ['bag', 0.155], ['stuff', 0.151], ['fly', 0.15], ['gun', 0.147], ['son', 0.145], ['yard', 0.143], ['tv', 0.142], ['apartment', 0.142]]\n",
      "\n",
      "Topic  4\n",
      "[['baby', 1.068], ['husband', 0.416], ['sister', 0.373], ['daughter', 0.324], ['doctor', 0.298], ['nurse', 0.267], ['son', 0.251], ['grandmother', 0.24], ['hospital', 0.203], ['parent', 0.14], ['law', 0.136], ['marry', 0.118], ['skin', 0.117], ['brother', 0.114], ['diaper', 0.112], ['doll', 0.108], ['scream', 0.104], ['wake', 0.104], ['milk', 0.102], ['blood', 0.102]]\n",
      "\n",
      "Topic  5\n",
      "[['dog', 1.526], ['doctor', 0.151], ['porch', 0.126], ['gate', 0.114], ['yard', 0.109], ['animal', 0.105], ['bark', 0.101], ['pet', 0.097], ['town', 0.087], ['mask', 0.083], ['bird', 0.083], ['road', 0.082], ['fear', 0.082], ['paw', 0.08], ['rain', 0.077], ['tail', 0.076], ['killer', 0.071], ['shovel', 0.069], ['blood', 0.069], ['fur', 0.069]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NMF + TF-IDF + nouns + verbs only\n",
    "docs = corpus_df.TEXT_NOUNS_VERBS\n",
    "tf = TfidfVectorizer(stop_words='english', min_df = 0.05, max_df=0.60)\n",
    "doc_term = tf.fit_transform(docs)\n",
    "nmf = NMF(n_components=5, init='nndsvda', max_iter=450)\n",
    "nmf.fit(doc_term)\n",
    "\n",
    "output = display_topics(nmf, tf.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7a8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "[['student', 0.298], ['husband', 0.275], ['daughter', 0.249], ['teacher', 0.24], ['class', 0.23], ['writer', 0.209], ['marriage', 0.197], ['son', 0.183], ['letter', 0.18], ['marry', 0.175], ['college', 0.171], ['novel', 0.168], ['painting', 0.164], ['city', 0.161], ['poem', 0.159], ['certain', 0.154], ['office', 0.154], ['conversation', 0.153], ['art', 0.153], ['study', 0.151], ['reply', 0.15], ['train', 0.149], ['death', 0.149], ['apartment', 0.146], ['wish', 0.145]]\n",
      "\n",
      "Topic  2\n",
      "[['bus', 0.23], ['box', 0.207], ['bottle', 0.201], ['bedroom', 0.198], ['tea', 0.191], ['coat', 0.191], ['kiss', 0.186], ['apartment', 0.182], ['bathroom', 0.18], ['coffee', 0.176], ['wine', 0.168], ['bar', 0.165], ['cat', 0.162], ['mirror', 0.162], ['plate', 0.157], ['rain', 0.157], ['sofa', 0.155], ['hall', 0.152], ['jacket', 0.147], ['paint', 0.145], ['desk', 0.143], ['lift', 0.141], ['towel', 0.141], ['shop', 0.14], ['shoe', 0.137]]\n",
      "\n",
      "Topic  3\n",
      "[['guy', 0.653], ['mom', 0.359], ['fuck', 0.331], ['brother', 0.3], ['fucking', 0.292], ['shit', 0.269], ['cop', 0.219], ['dad', 0.216], ['ass', 0.214], ['hit', 0.183], ['movie', 0.176], ['kill', 0.157], ['fly', 0.151], ['nice', 0.147], ['gun', 0.14], ['dollar', 0.135], ['apartment', 0.134], ['son', 0.133], ['girlfriend', 0.13], ['tv', 0.128], ['stuff', 0.126], ['hurt', 0.126], ['beer', 0.124], ['hell', 0.124], ['yell', 0.123]]\n",
      "\n",
      "Topic  4\n",
      "[['baby', 1.019], ['husband', 0.389], ['sister', 0.341], ['daughter', 0.313], ['doctor', 0.276], ['nurse', 0.269], ['son', 0.238], ['grandmother', 0.236], ['hospital', 0.19], ['law', 0.144], ['marry', 0.116], ['diaper', 0.112], ['pregnant', 0.105], ['scream', 0.103], ['stroller', 0.099], ['blood', 0.097], ['doll', 0.096], ['brother', 0.095], ['aunt', 0.094], ['grandfather', 0.093], ['breast', 0.089], ['milk', 0.087], ['birth', 0.085], ['uncle', 0.084], ['belly', 0.083]]\n",
      "\n",
      "Topic  5\n",
      "[['dog', 1.485], ['doctor', 0.13], ['porch', 0.125], ['leash', 0.116], ['pet', 0.114], ['yard', 0.109], ['gate', 0.108], ['animal', 0.095], ['mask', 0.087], ['road', 0.085], ['bark', 0.084], ['fear', 0.083], ['rain', 0.078], ['paw', 0.077], ['bird', 0.074], ['fuckin', 0.073], ['killer', 0.073], ['shovel', 0.072], ['vet', 0.07], ['tail', 0.07], ['rabbit', 0.069], ['fur', 0.066], ['fence', 0.065], ['film', 0.065], ['blood', 0.063]]\n",
      "\n",
      "Topic  6\n",
      "[['road', 0.371], ['river', 0.259], ['field', 0.24], ['horse', 0.236], ['village', 0.232], ['sea', 0.222], ['wind', 0.211], ['stone', 0.21], ['sun', 0.202], ['boat', 0.2], ['fish', 0.197], ['sky', 0.183], ['gun', 0.179], ['rock', 0.178], ['land', 0.177], ['wood', 0.175], ['truck', 0.173], ['grass', 0.172], ['bird', 0.171], ['mountain', 0.17], ['ground', 0.167], ['beach', 0.162], ['farm', 0.154], ['hill', 0.15], ['cabin', 0.149]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NMF + TF-IDF + nouns + verbs + adjs\n",
    "docs = corpus_df.TEXT_NOUNS_VERBS_ADJS\n",
    "tf = TfidfVectorizer(stop_words='english', min_df = 0.02, max_df=0.55)\n",
    "doc_term = tf.fit_transform(docs) \n",
    "nmf = NMF(n_components=6, init='nndsvda', max_iter=450)\n",
    "NMF_doc_topic = nmf.fit_transform(doc_term)\n",
    "\n",
    "output = display_topics(nmf, tf.get_feature_names(), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc663bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.101</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>0.037</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>0.037</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>0.107</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>944 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5\n",
       "0    0.043  0.096  0.013  0.058  0.077  0.045\n",
       "1    0.005  0.090  0.020  0.182  0.000  0.000\n",
       "2    0.094  0.099  0.012  0.000  0.000  0.000\n",
       "3    0.101  0.009  0.078  0.000  0.016  0.027\n",
       "4    0.049  0.031  0.000  0.054  0.015  0.005\n",
       "..     ...    ...    ...    ...    ...    ...\n",
       "939  0.035  0.058  0.002  0.035  0.008  0.060\n",
       "940  0.037  0.120  0.070  0.005  0.000  0.000\n",
       "941  0.037  0.172  0.009  0.026  0.000  0.007\n",
       "942  0.000  0.000  0.005  0.136  0.000  0.094\n",
       "943  0.107  0.059  0.000  0.065  0.011  0.018\n",
       "\n",
       "[944 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at topics for each doc\n",
    "NMF_doc_topic = nmf.fit_transform(doc_term)\n",
    "\n",
    "NMF_doc_topic_df = pd.DataFrame(NMF_doc_topic.round(3),\n",
    "                 columns = [ix for ix, val in enumerate(nmf.components_)]\n",
    "                )\n",
    "NMF_doc_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a95670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abduct</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abrupt</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>élite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6762 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  abandonment  abdomen  abduct  abide   ability  abortion    abrupt  \\\n",
       "0      0.0          0.0      0.0     0.0    0.0  0.034527       0.0  0.000000   \n",
       "1      0.0          0.0      0.0     0.0    0.0  0.000000       0.0  0.000000   \n",
       "2      0.0          0.0      0.0     0.0    0.0  0.000000       0.0  0.039368   \n",
       "3      0.0          0.0      0.0     0.0    0.0  0.000000       0.0  0.000000   \n",
       "4      0.0          0.0      0.0     0.0    0.0  0.000000       0.0  0.000000   \n",
       "\n",
       "    absence    absent  ...     youth  youthful  zigzag  zip  zipper  zombie  \\\n",
       "0  0.000000  0.020317  ...  0.031073       0.0     0.0  0.0     0.0     0.0   \n",
       "1  0.000000  0.000000  ...  0.000000       0.0     0.0  0.0     0.0     0.0   \n",
       "2  0.000000  0.000000  ...  0.000000       0.0     0.0  0.0     0.0     0.0   \n",
       "3  0.035504  0.000000  ...  0.034431       0.0     0.0  0.0     0.0     0.0   \n",
       "4  0.000000  0.000000  ...  0.031750       0.0     0.0  0.0     0.0     0.0   \n",
       "\n",
       "   zone  zoo  zoom  élite  \n",
       "0   0.0  0.0   0.0    0.0  \n",
       "1   0.0  0.0   0.0    0.0  \n",
       "2   0.0  0.0   0.0    0.0  \n",
       "3   0.0  0.0   0.0    0.0  \n",
       "4   0.0  0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 6762 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_df = pd.DataFrame(doc_term.toarray(), columns=tf.get_feature_names())\n",
    "doc_term_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0140211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 779 THE ALBANIAN WRITERS' UNION AS MIRRORED BY A WOMAN ISMAIL KADARE\n",
      "Topic 2 486 SWEET DREAMS PETER STAMM\n",
      "Topic 3 22 THE MOM OF BOLD ACTION GEORGE SAUNDERS\n",
      "Topic 4 58 THE WINGED THING PATRICIA LOCKWOOD\n",
      "Topic 5 555 THE YELLOW SAMANTHA HUNT\n",
      "Topic 6 490 TRANSATLANTIC COLUM MCCANN\n"
     ]
    }
   ],
   "source": [
    "#looking at top doc for each topic\n",
    "for ix, val in enumerate(nmf.components_):\n",
    "    max_story = np.argmax(NMF_doc_topic_df[ix])\n",
    "    print('Topic', ix+1, max_story, corpus_df.TITLE[max_story], corpus_df.AUTHOR[max_story])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feab6101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "husband      0.298740\n",
       "son          0.297999\n",
       "terrace      0.295327\n",
       "grandson     0.269114\n",
       "daughter     0.267363\n",
       "doll         0.250222\n",
       "baby         0.174818\n",
       "gurney       0.127792\n",
       "toast        0.111103\n",
       "whistle      0.093893\n",
       "church       0.091348\n",
       "birth        0.091123\n",
       "doctor       0.090895\n",
       "pool         0.089675\n",
       "oil          0.084820\n",
       "strap        0.078126\n",
       "champagne    0.077757\n",
       "devil        0.077366\n",
       "lower        0.077176\n",
       "wrap         0.076927\n",
       "Name: 213, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at top terms for Sunrise, Sunset\n",
    "(doc_term_df.loc[213].sort_values(ascending=False)).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "695b8fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 944 entries, 0 to 943\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   URL                    944 non-null    object\n",
      " 1   TITLE                  944 non-null    object\n",
      " 2   AUTHOR                 944 non-null    object\n",
      " 3   DATE                   944 non-null    object\n",
      " 4   TEXT                   944 non-null    object\n",
      " 5   WORDCOUNT              944 non-null    int64 \n",
      " 6   TEXT_NOUNS             944 non-null    object\n",
      " 7   TEXT_NOUNS_ADJS        944 non-null    object\n",
      " 8   TEXT_NOUNS_VERBS       944 non-null    object\n",
      " 9   TEXT_NOUNS_VERBS_ADJS  944 non-null    object\n",
      " 10  TEXT_ENTS              944 non-null    object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 81.2+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b798abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 944 entries, 0 to 943\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       944 non-null    float64\n",
      " 1   1       944 non-null    float64\n",
      " 2   2       944 non-null    float64\n",
      " 3   3       944 non-null    float64\n",
      " 4   4       944 non-null    float64\n",
      " 5   5       944 non-null    float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 44.4 KB\n"
     ]
    }
   ],
   "source": [
    "NMF_doc_topic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87346390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000\n",
       "1    0.044\n",
       "2    0.000\n",
       "3    0.000\n",
       "4    0.000\n",
       "5    0.199\n",
       "Name: 490, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_doc_topic_df.loc[490]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2f2b270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.047</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.113</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>0.065</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>0.099</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>0.085</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>944 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5\n",
       "0    0.047  0.064  0.011  0.023  0.088  0.093\n",
       "1    0.044  0.022  0.012  0.259  0.000  0.013\n",
       "2    0.131  0.018  0.000  0.000  0.001  0.026\n",
       "3    0.113  0.025  0.039  0.000  0.013  0.020\n",
       "4    0.011  0.010  0.000  0.000  0.017  0.151\n",
       "..     ...    ...    ...    ...    ...    ...\n",
       "939  0.030  0.074  0.019  0.038  0.015  0.046\n",
       "940  0.065  0.038  0.063  0.018  0.000  0.007\n",
       "941  0.099  0.049  0.000  0.033  0.000  0.055\n",
       "942  0.000  0.062  0.000  0.062  0.001  0.097\n",
       "943  0.085  0.030  0.000  0.064  0.012  0.074\n",
       "\n",
       "[944 rows x 6 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best topics\n",
    "\n",
    "#NMF + TF-IDF + nouns only\n",
    "\n",
    "docs = corpus_df.TEXT_NOUNS\n",
    "tf = TfidfVectorizer(stop_words='english', min_df = 0.03, max_df=0.55)\n",
    "doc_term = tf.fit_transform(docs)\n",
    "nmf = NMF(n_components=6, init='nndsvda', max_iter=450)\n",
    "\n",
    "NMF_doc_topic = nmf.fit_transform(doc_term)\n",
    "\n",
    "NMF_doc_topic_df = pd.DataFrame(NMF_doc_topic.round(3),\n",
    "                 columns = [ix for ix, val in enumerate(nmf.components_)]\n",
    "                )\n",
    "NMF_doc_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3be2427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle\n",
    "NMF_doc_topic_df.to_pickle(\"NMF_doc_topic_df3.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca33127",
   "metadata": {},
   "source": [
    "## LSA \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0224a266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "[['dog', 0.109], ['husband', 0.107], ['son', 0.107], ['kid', 0.107], ['baby', 0.105], ['guy', 0.098], ['brother', 0.093], ['daughter', 0.092], ['apartment', 0.089], ['sister', 0.085], ['road', 0.08], ['doctor', 0.077], ['bar', 0.07], ['city', 0.07], ['dinner', 0.069], ['coffee', 0.067], ['office', 0.064], ['box', 0.063], ['bus', 0.063], ['party', 0.062]]\n",
      "\n",
      "Topic  2\n",
      "[['dog', 0.389], ['road', 0.192], ['river', 0.109], ['gun', 0.104], ['yard', 0.101], ['horse', 0.101], ['field', 0.094], ['truck', 0.088], ['rock', 0.088], ['wind', 0.085], ['sun', 0.084], ['grass', 0.082], ['kid', 0.082], ['wood', 0.08], ['bird', 0.076], ['fence', 0.072], ['stone', 0.071], ['cabin', 0.071], ['porch', 0.069], ['cow', 0.068]]\n",
      "\n",
      "Topic  3\n",
      "[['baby', 0.421], ['kid', 0.352], ['guy', 0.294], ['mom', 0.209], ['dog', 0.167], ['dad', 0.103], ['cop', 0.099], ['shit', 0.098], ['sister', 0.091], ['ass', 0.089], ['doctor', 0.089], ['nurse', 0.076], ['apartment', 0.074], ['movie', 0.068], ['couch', 0.066], ['dollar', 0.058], ['diaper', 0.058], ['daughter', 0.054], ['lady', 0.054], ['hospital', 0.053]]\n",
      "\n",
      "Topic  4\n",
      "[['baby', 0.446], ['husband', 0.278], ['daughter', 0.207], ['sister', 0.205], ['village', 0.149], ['son', 0.136], ['doctor', 0.135], ['grandmother', 0.133], ['nurse', 0.103], ['grandfather', 0.093], ['law', 0.084], ['hospital', 0.083], ['garden', 0.075], ['uncle', 0.067], ['brother', 0.067], ['church', 0.065], ['cousin', 0.062], ['tea', 0.06], ['wedding', 0.059], ['doll', 0.058]]\n",
      "\n",
      "Topic  5\n",
      "[['dog', 0.758], ['husband', 0.153], ['doctor', 0.094], ['apartment', 0.093], ['film', 0.075], ['painting', 0.063], ['daughter', 0.061], ['mask', 0.051], ['wine', 0.047], ['walk', 0.045], ['taxi', 0.044], ['gate', 0.042], ['tea', 0.041], ['office', 0.04], ['bottle', 0.038], ['party', 0.038], ['coat', 0.038], ['paw', 0.037], ['wedding', 0.037], ['pet', 0.036]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LSA with TF-IDF\n",
    "#looking at nouns only\n",
    "docs = corpus_df.TEXT_NOUNS\n",
    "tf = TfidfVectorizer(stop_words='english', min_df=0.05, max_df=0.55)\n",
    "doc_term = tf.fit_transform(docs)\n",
    "lsa = TruncatedSVD(5)\n",
    "lsa.fit(doc_term)\n",
    "topic_term = lsa.components_\n",
    "output = display_topics(model=lsa, feature_names=tf.get_feature_names(), no_top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "891d1f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "[['dog', 0.097], ['baby', 0.095], ['husband', 0.095], ['kid', 0.093], ['son', 0.092], ['brother', 0.084], ['guy', 0.082], ['apartment', 0.079], ['daughter', 0.079], ['sister', 0.076], ['road', 0.072], ['doctor', 0.068], ['dinner', 0.061], ['city', 0.06], ['bar', 0.06], ['coffee', 0.058], ['bus', 0.056], ['box', 0.056], ['office', 0.055], ['bottle', 0.055]]\n",
      "\n",
      "Topic  2\n",
      "[['dog', 0.386], ['road', 0.16], ['river', 0.093], ['yard', 0.086], ['horse', 0.084], ['gun', 0.083], ['kid', 0.081], ['rock', 0.08], ['field', 0.079], ['sun', 0.075], ['wind', 0.074], ['truck', 0.074], ['cabin', 0.074], ['wood', 0.072], ['porch', 0.069], ['grass', 0.067], ['stone', 0.065], ['boat', 0.064], ['fish', 0.064], ['fence', 0.061]]\n",
      "\n",
      "Topic  3\n",
      "[['baby', 0.347], ['kid', 0.31], ['guy', 0.263], ['mom', 0.193], ['dog', 0.12], ['cop', 0.095], ['shit', 0.092], ['dad', 0.088], ['apartment', 0.087], ['fucking', 0.079], ['ass', 0.079], ['brother', 0.068], ['doctor', 0.067], ['movie', 0.065], ['nurse', 0.064], ['son', 0.064], ['couch', 0.063], ['husband', 0.062], ['dollar', 0.058], ['nice', 0.058]]\n",
      "\n",
      "Topic  4\n",
      "[['baby', 0.462], ['sister', 0.187], ['husband', 0.159], ['grandmother', 0.146], ['son', 0.125], ['daughter', 0.119], ['nurse', 0.112], ['village', 0.109], ['doctor', 0.102], ['grandfather', 0.084], ['hospital', 0.083], ['law', 0.073], ['uncle', 0.067], ['aunt', 0.065], ['church', 0.059], ['doll', 0.059], ['brother', 0.057], ['dress', 0.052], ['cousin', 0.051], ['milk', 0.05]]\n",
      "\n",
      "Topic  5\n",
      "[['dog', 0.665], ['husband', 0.231], ['daughter', 0.128], ['doctor', 0.097], ['baby', 0.07], ['painting', 0.063], ['tea', 0.052], ['gate', 0.051], ['apartment', 0.05], ['leash', 0.049], ['grandmother', 0.049], ['wedding', 0.046], ['piano', 0.045], ['film', 0.045], ['painter', 0.044], ['nurse', 0.044], ['hospital', 0.042], ['garden', 0.041], ['sister', 0.04], ['marriage', 0.035]]\n",
      "\n",
      "Topic  6\n",
      "[['baby', 0.137], ['cat', 0.132], ['apartment', 0.128], ['bar', 0.112], ['drink', 0.109], ['bottle', 0.103], ['wine', 0.095], ['bathroom', 0.087], ['bus', 0.078], ['bedroom', 0.075], ['sofa', 0.073], ['towel', 0.07], ['box', 0.069], ['couch', 0.063], ['coffee', 0.062], ['coat', 0.061], ['mirror', 0.061], ['party', 0.06], ['sink', 0.058], ['plate', 0.057]]\n",
      "\n",
      "Topic  7\n",
      "[['baby', 0.332], ['poem', 0.112], ['sea', 0.107], ['film', 0.093], ['nurse', 0.091], ['beach', 0.088], ['doctor', 0.081], ['river', 0.07], ['writer', 0.068], ['boat', 0.068], ['human', 0.067], ['poet', 0.057], ['movie', 0.057], ['wave', 0.052], ['rock', 0.051], ['moon', 0.05], ['angel', 0.05], ['feeling', 0.049], ['hotel', 0.048], ['patient', 0.048]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#nouns plus adjectives\n",
    "docs = corpus_df.TEXT_NOUNS_ADJS\n",
    "tf = TfidfVectorizer(stop_words='english', max_df=0.55)\n",
    "doc_term = tf.fit_transform(docs)\n",
    "lsa = TruncatedSVD(7)\n",
    "lsa.fit(doc_term)\n",
    "topic_term = lsa.components_\n",
    "output = display_topics(model=lsa, feature_names=tf.get_feature_names(), no_top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6c23ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "[['dog', 0.093], ['husband', 0.091], ['baby', 0.089], ['son', 0.089], ['brother', 0.081], ['guy', 0.078], ['apartment', 0.076], ['daughter', 0.075], ['sister', 0.072], ['road', 0.069], ['doctor', 0.065], ['bar', 0.059], ['city', 0.059], ['dinner', 0.058], ['coffee', 0.056], ['kiss', 0.056], ['bus', 0.054], ['box', 0.054], ['dream', 0.053], ['office', 0.053]]\n",
      "\n",
      "Topic  2\n",
      "[['dog', 0.311], ['road', 0.145], ['river', 0.087], ['gun', 0.08], ['yard', 0.077], ['sun', 0.074], ['rock', 0.072], ['horse', 0.071], ['fish', 0.07], ['truck', 0.07], ['field', 0.07], ['wind', 0.068], ['rain', 0.065], ['wood', 0.064], ['cabin', 0.063], ['boat', 0.061], ['grass', 0.061], ['porch', 0.059], ['stone', 0.057], ['fence', 0.056]]\n",
      "\n",
      "Topic  3\n",
      "[['baby', 0.442], ['guy', 0.195], ['mom', 0.174], ['husband', 0.119], ['doctor', 0.107], ['apartment', 0.104], ['nurse', 0.099], ['sister', 0.094], ['daughter', 0.094], ['son', 0.089], ['fuck', 0.089], ['shit', 0.082], ['brother', 0.079], ['dad', 0.077], ['hospital', 0.075], ['couch', 0.073], ['cop', 0.072], ['ass', 0.07], ['grandmother', 0.07], ['dollar', 0.062]]\n",
      "\n",
      "Topic  4\n",
      "[['dog', 0.465], ['guy', 0.188], ['apartment', 0.118], ['movie', 0.09], ['film', 0.082], ['bar', 0.08], ['painting', 0.08], ['writer', 0.071], ['poem', 0.065], ['fuck', 0.064], ['mail', 0.064], ['building', 0.053], ['hotel', 0.052], ['cop', 0.051], ['painter', 0.051], ['cat', 0.048], ['text', 0.046], ['restaurant', 0.045], ['shit', 0.044], ['tv', 0.044]]\n",
      "\n",
      "Topic  5\n",
      "[['dog', 0.636], ['husband', 0.237], ['daughter', 0.137], ['doctor', 0.111], ['baby', 0.091], ['sister', 0.083], ['tea', 0.07], ['garden', 0.064], ['gate', 0.058], ['wedding', 0.057], ['grandmother', 0.057], ['nurse', 0.057], ['marry', 0.055], ['hospital', 0.048], ['village', 0.047], ['piano', 0.047], ['marriage', 0.046], ['leash', 0.044], ['teacher', 0.043], ['doll', 0.038]]\n",
      "\n",
      "Topic  6\n",
      "[['apartment', 0.092], ['painting', 0.092], ['bathroom', 0.091], ['bottle', 0.089], ['bedroom', 0.087], ['kiss', 0.086], ['mirror', 0.086], ['sofa', 0.084], ['wine', 0.083], ['baby', 0.082], ['box', 0.074], ['towel', 0.073], ['coat', 0.073], ['tea', 0.068], ['paint', 0.066], ['hall', 0.065], ['bus', 0.064], ['plate', 0.062], ['coffee', 0.062], ['rain', 0.061]]\n",
      "\n",
      "Topic  7\n",
      "[['baby', 0.421], ['poem', 0.146], ['writer', 0.107], ['doctor', 0.103], ['nurse', 0.095], ['sea', 0.092], ['film', 0.087], ['fish', 0.08], ['river', 0.077], ['poet', 0.075], ['beach', 0.065], ['novel', 0.065], ['boat', 0.062], ['patient', 0.054], ['rock', 0.053], ['student', 0.052], ['moon', 0.05], ['movie', 0.049], ['diaper', 0.049], ['stroller', 0.047]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#trying nouns and verbs\n",
    "docs = corpus_df.TEXT_NOUNS_VERBS\n",
    "tf = TfidfVectorizer(stop_words='english', max_df=0.55)\n",
    "doc_term = tf.fit_transform(docs)\n",
    "lsa = TruncatedSVD(7)\n",
    "lsa.fit(doc_term)\n",
    "topic_term = lsa.components_\n",
    "output = display_topics(model=lsa, feature_names=tf.get_feature_names(), no_top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de17e21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "[['dog', 0.086], ['baby', 0.084], ['husband', 0.083], ['son', 0.082], ['brother', 0.074], ['guy', 0.072], ['apartment', 0.069], ['daughter', 0.069], ['sister', 0.067], ['road', 0.064], ['doctor', 0.06], ['bar', 0.054], ['city', 0.053], ['dinner', 0.053], ['coffee', 0.052], ['kiss', 0.051], ['bus', 0.05], ['box', 0.05], ['dream', 0.049], ['office', 0.049]]\n",
      "\n",
      "Topic  2\n",
      "[['dog', 0.279], ['road', 0.135], ['river', 0.081], ['gun', 0.076], ['sun', 0.073], ['yard', 0.071], ['rock', 0.07], ['truck', 0.069], ['fish', 0.068], ['wind', 0.068], ['horse', 0.067], ['field', 0.067], ['rain', 0.064], ['cabin', 0.063], ['wood', 0.062], ['boat', 0.061], ['baby', 0.059], ['grass', 0.059], ['stone', 0.055], ['porch', 0.054]]\n",
      "\n",
      "Topic  3\n",
      "[['baby', 0.349], ['guy', 0.236], ['mom', 0.18], ['fuck', 0.114], ['apartment', 0.112], ['shit', 0.098], ['fucking', 0.092], ['brother', 0.09], ['dad', 0.085], ['cop', 0.083], ['ass', 0.081], ['doctor', 0.079], ['nurse', 0.076], ['couch', 0.073], ['son', 0.072], ['sister', 0.072], ['husband', 0.071], ['movie', 0.067], ['dollar', 0.063], ['daughter', 0.06]]\n",
      "\n",
      "Topic  4\n",
      "[['baby', 0.414], ['husband', 0.198], ['sister', 0.174], ['grandmother', 0.135], ['daughter', 0.131], ['village', 0.12], ['son', 0.11], ['nurse', 0.092], ['grandfather', 0.08], ['doctor', 0.08], ['law', 0.077], ['hospital', 0.07], ['tea', 0.066], ['doll', 0.061], ['uncle', 0.061], ['church', 0.06], ['garden', 0.059], ['marry', 0.054], ['aunt', 0.054], ['milk', 0.053]]\n",
      "\n",
      "Topic  5\n",
      "[['dog', 0.704], ['husband', 0.178], ['doctor', 0.082], ['daughter', 0.082], ['tea', 0.061], ['gate', 0.054], ['leash', 0.052], ['painter', 0.051], ['film', 0.046], ['painting', 0.046], ['pet', 0.045], ['apartment', 0.045], ['garden', 0.043], ['fuckin', 0.041], ['wedding', 0.041], ['wine', 0.039], ['porch', 0.038], ['bedroom', 0.037], ['marry', 0.037], ['drawer', 0.037]]\n",
      "\n",
      "Topic  6\n",
      "[['apartment', 0.094], ['painting', 0.09], ['bathroom', 0.089], ['bottle', 0.088], ['wine', 0.081], ['kiss', 0.079], ['sofa', 0.077], ['bedroom', 0.076], ['cat', 0.075], ['mirror', 0.075], ['bar', 0.074], ['towel', 0.068], ['box', 0.067], ['coat', 0.062], ['paint', 0.061], ['coffee', 0.061], ['bus', 0.06], ['hotel', 0.059], ['party', 0.057], ['film', 0.056]]\n",
      "\n",
      "Topic  7\n",
      "[['baby', 0.382], ['sea', 0.109], ['beach', 0.105], ['poem', 0.099], ['boat', 0.081], ['human', 0.068], ['swim', 0.068], ['nurse', 0.065], ['doctor', 0.06], ['river', 0.06], ['rock', 0.059], ['fish', 0.057], ['movie', 0.053], ['lake', 0.052], ['film', 0.05], ['feeling', 0.05], ['hotel', 0.049], ['sand', 0.049], ['shore', 0.049], ['moon', 0.048]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LSA with nouns, verbs, adjectives\n",
    "docs = corpus_df.TEXT_NOUNS_VERBS_ADJS\n",
    "tf = TfidfVectorizer(stop_words='english', max_df=0.55)\n",
    "doc_term = tf.fit_transform(docs)\n",
    "lsa = TruncatedSVD(7)\n",
    "lsa.fit(doc_term)\n",
    "topic_term = lsa.components_\n",
    "output = display_topics(model=lsa, feature_names=tf.get_feature_names(), no_top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5a96ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TITLE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANNUNCIATION</th>\n",
       "      <td>0.362</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ONCE REMOVED</th>\n",
       "      <td>0.318</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LONG DISTANCE</th>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHAT'S THE DEAL, HUMMINGBIRD?</th>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIREWORKS</th>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE GUARDIANS</th>\n",
       "      <td>0.252</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U.F.O. IN KUSHIRO</th>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOBODY'S BUSINESS</th>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE SHAWL</th>\n",
       "      <td>0.242</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHAT IS REMEMBERED</th>\n",
       "      <td>0.335</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>944 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0      1      2      3      4      5      6\n",
       "TITLE                                                                         \n",
       "ANNUNCIATION                   0.362  0.040 -0.023  0.039  0.108  0.031 -0.013\n",
       "ONCE REMOVED                   0.318  0.015  0.137  0.162 -0.013  0.090  0.103\n",
       "LONG DISTANCE                  0.262 -0.109 -0.027 -0.034  0.024  0.084 -0.032\n",
       "WHAT'S THE DEAL, HUMMINGBIRD?  0.272 -0.081  0.014 -0.101 -0.022 -0.035  0.028\n",
       "FIREWORKS                      0.184 -0.047 -0.005  0.069  0.044 -0.022 -0.031\n",
       "...                              ...    ...    ...    ...    ...    ...    ...\n",
       "THE GUARDIANS                  0.252  0.023 -0.049  0.051 -0.007 -0.001  0.014\n",
       "U.F.O. IN KUSHIRO              0.266 -0.023  0.022 -0.039 -0.028  0.093 -0.072\n",
       "NOBODY'S BUSINESS              0.338 -0.032 -0.008  0.043 -0.016  0.137 -0.074\n",
       "THE SHAWL                      0.242  0.098  0.038  0.160 -0.026 -0.115  0.000\n",
       "WHAT IS REMEMBERED             0.335 -0.103 -0.046  0.082  0.058  0.013  0.041\n",
       "\n",
       "[944 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic = lsa.fit_transform(doc_term)\n",
    "\n",
    "doc_topic_df = pd.DataFrame(doc_topic.round(3),\n",
    "                 index = corpus_df.TITLE,\n",
    "                 columns = [ix for ix, val in enumerate(lsa.components_)]\n",
    "                )\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "092ea196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 ONCE IN A LIFETIME JHUMPA LAHIRI 0.08594190554048627\n",
      "516 DOG RUN MOON CALLAN WINK 0.2747004896453223\n",
      "529 HOME GEORGE SAUNDERS 0.3479108312470661\n",
      "394 A SHELTERED WOMAN YIYUN LI 0.41269616737029624\n",
      "695 THE DOG RODDY DOYLE 0.7428194402995678\n",
      "809 SOLACE DONALD ANTRIM 0.10623639330303336\n",
      "838 MIRACLE JUDY BUDNITZ 0.41348752836494407\n"
     ]
    }
   ],
   "source": [
    "#looking at top doc for each topic\n",
    "for ix, val in enumerate(lsa.components_):\n",
    "    max_story = np.argmax(doc_topic_df[ix], axis=-1)\n",
    "    print(max_story, corpus_df.TITLE[max_story], corpus_df.AUTHOR[max_story], np.max(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a201cce",
   "metadata": {},
   "source": [
    "## LDA\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b2d9492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"cat\" + 0.005*\"husband\" + 0.004*\"bottle\" + 0.004*\"box\" + 0.004*\"bedroom\" + 0.004*\"sister\" + 0.004*\"son\" + 0.004*\"skin\" + 0.003*\"dinner\" + 0.003*\"coffee\"'),\n",
       " (1,\n",
       "  '0.023*\"fish\" + 0.022*\"sister\" + 0.008*\"cow\" + 0.006*\"niece\" + 0.005*\"boat\" + 0.005*\"milk\" + 0.005*\"nest\" + 0.004*\"cat\" + 0.004*\"gangster\" + 0.004*\"uncle\"'),\n",
       " (2,\n",
       "  '0.012*\"kid\" + 0.010*\"guy\" + 0.009*\"baby\" + 0.005*\"mom\" + 0.005*\"brother\" + 0.005*\"son\" + 0.004*\"apartment\" + 0.003*\"dollar\" + 0.003*\"bus\" + 0.003*\"doctor\"'),\n",
       " (3,\n",
       "  '0.006*\"husband\" + 0.004*\"student\" + 0.004*\"daughter\" + 0.004*\"class\" + 0.003*\"teacher\" + 0.003*\"son\" + 0.003*\"city\" + 0.003*\"conversation\" + 0.003*\"death\" + 0.003*\"apartment\"'),\n",
       " (4,\n",
       "  '0.007*\"road\" + 0.003*\"summer\" + 0.003*\"bird\" + 0.003*\"monkey\" + 0.003*\"field\" + 0.003*\"rain\" + 0.003*\"tea\" + 0.003*\"bar\" + 0.002*\"wind\" + 0.002*\"train\"'),\n",
       " (5,\n",
       "  '0.006*\"road\" + 0.005*\"sun\" + 0.005*\"sea\" + 0.004*\"sky\" + 0.004*\"stone\" + 0.004*\"gun\" + 0.004*\"wind\" + 0.004*\"ground\" + 0.004*\"beach\" + 0.003*\"horse\"'),\n",
       " (6,\n",
       "  '0.038*\"dog\" + 0.008*\"painter\" + 0.006*\"road\" + 0.005*\"daughter\" + 0.003*\"bottle\" + 0.003*\"apartment\" + 0.003*\"village\" + 0.003*\"garden\" + 0.003*\"beast\" + 0.003*\"gate\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora, models, matutils\n",
    "\n",
    "#looking at nouns only\n",
    "docs = corpus_df.TEXT_NOUNS\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.55)\n",
    "term_doc_matrix = vectorizer.fit_transform(docs).transpose()\n",
    "\n",
    "corpus = matutils.Sparse2Corpus(term_doc_matrix)\n",
    "id2word = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=7, id2word=id2word, passes=150)\n",
    "\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e71e6b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"dog\" + 0.006*\"stone\" + 0.005*\"sun\" + 0.005*\"sea\" + 0.005*\"wind\" + 0.005*\"sky\" + 0.005*\"rise\" + 0.005*\"bird\" + 0.004*\"rock\" + 0.004*\"village\"'),\n",
       " (1,\n",
       "  '0.011*\"baby\" + 0.007*\"husband\" + 0.007*\"sister\" + 0.006*\"doctor\" + 0.005*\"bag\" + 0.005*\"dog\" + 0.005*\"food\" + 0.005*\"son\" + 0.005*\"box\" + 0.004*\"parent\"'),\n",
       " (2,\n",
       "  '0.014*\"road\" + 0.010*\"town\" + 0.007*\"truck\" + 0.006*\"gun\" + 0.006*\"fish\" + 0.006*\"horse\" + 0.005*\"yard\" + 0.005*\"ground\" + 0.005*\"field\" + 0.004*\"driver\"'),\n",
       " (3,\n",
       "  '0.006*\"parent\" + 0.006*\"husband\" + 0.006*\"son\" + 0.006*\"daughter\" + 0.004*\"student\" + 0.004*\"class\" + 0.004*\"learn\" + 0.004*\"death\" + 0.004*\"teacher\" + 0.004*\"marry\"'),\n",
       " (4,\n",
       "  '0.018*\"kid\" + 0.017*\"guy\" + 0.007*\"fuck\" + 0.007*\"mom\" + 0.006*\"movie\" + 0.005*\"hit\" + 0.005*\"shit\" + 0.005*\"dad\" + 0.005*\"bar\" + 0.005*\"brother\"'),\n",
       " (5,\n",
       "  '0.005*\"painting\" + 0.004*\"hotel\" + 0.004*\"train\" + 0.004*\"paint\" + 0.004*\"evening\" + 0.004*\"bar\" + 0.003*\"mirror\" + 0.003*\"coffee\" + 0.003*\"apartment\" + 0.003*\"clothe\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nouns and verbs\n",
    "docs = corpus_df.TEXT_NOUNS_VERBS\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df = 0.05, max_df=0.60)\n",
    "term_doc_matrix = vectorizer.fit_transform(docs).transpose()\n",
    "\n",
    "corpus = matutils.Sparse2Corpus(term_doc_matrix)\n",
    "id2word = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=6, id2word=id2word, passes=250)\n",
    "\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd8aa393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"dog\" + 0.006*\"road\" + 0.003*\"wind\" + 0.003*\"town\" + 0.003*\"sun\" + 0.003*\"yard\" + 0.003*\"truck\" + 0.003*\"ground\" + 0.003*\"field\" + 0.003*\"cat\"'),\n",
       " (1,\n",
       "  '0.003*\"bedroom\" + 0.003*\"clothe\" + 0.003*\"parent\" + 0.003*\"bag\" + 0.003*\"evening\" + 0.002*\"bottle\" + 0.002*\"kiss\" + 0.002*\"tea\" + 0.002*\"husband\" + 0.002*\"press\"'),\n",
       " (2,\n",
       "  '0.004*\"human\" + 0.003*\"beach\" + 0.003*\"sea\" + 0.003*\"hotel\" + 0.002*\"death\" + 0.002*\"sky\" + 0.002*\"fear\" + 0.002*\"space\" + 0.002*\"form\" + 0.002*\"certain\"'),\n",
       " (3,\n",
       "  '0.008*\"apartment\" + 0.006*\"husband\" + 0.005*\"parent\" + 0.004*\"daughter\" + 0.004*\"student\" + 0.004*\"class\" + 0.004*\"building\" + 0.004*\"office\" + 0.004*\"movie\" + 0.004*\"bar\"'),\n",
       " (4,\n",
       "  '0.011*\"kid\" + 0.008*\"baby\" + 0.007*\"guy\" + 0.005*\"sister\" + 0.004*\"brother\" + 0.004*\"mom\" + 0.004*\"fuck\" + 0.004*\"doctor\" + 0.004*\"son\" + 0.003*\"bag\"'),\n",
       " (5,\n",
       "  '0.004*\"son\" + 0.003*\"town\" + 0.003*\"parent\" + 0.003*\"learn\" + 0.002*\"death\" + 0.002*\"grandfather\" + 0.002*\"country\" + 0.002*\"village\" + 0.002*\"letter\" + 0.002*\"certain\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nouns and verbs and adjs\n",
    "docs = corpus_df.TEXT_NOUNS_VERBS_ADJS\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df = 10, max_df=0.60)\n",
    "term_doc_matrix = vectorizer.fit_transform(docs).transpose()\n",
    "\n",
    "corpus = matutils.Sparse2Corpus(term_doc_matrix)\n",
    "id2word = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=6, id2word=id2word, passes=350)\n",
    "\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d89fb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.008445222),\n",
       " (1, 0.008445222),\n",
       " (2, 0.008445222),\n",
       " (3, 0.008445222),\n",
       " (4, 0.95777386),\n",
       " (5, 0.008445222)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe showing topic probability scores for each document\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def lda_df_maker(data, n_topics):\n",
    "    topic_doc_dist = defaultdict(list)\n",
    "    for ix, val in enumerate(data):\n",
    "        doc_top_list = lda.get_document_topics(corpus[ix], minimum_probability=0, minimum_phi_value=0)\n",
    "        for pair in doc_top_list:\n",
    "            topic_num = pair[0]\n",
    "            topic_score = pair[1]\n",
    "            topic_doc_dist[topic_num].append(np.round(topic_score, 3))\n",
    "    return topic_doc_dist\n",
    "\n",
    "lda.get_document_topics(corpus[2], minimum_probability=0, minimum_phi_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f94dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df_dict = lda_df_maker(corpus, n_topics=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "336906c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>944 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5\n",
       "0    0.007  0.007  0.007  0.007  0.965  0.007\n",
       "1    0.008  0.008  0.008  0.008  0.960  0.008\n",
       "2    0.008  0.008  0.008  0.008  0.958  0.008\n",
       "3    0.009  0.009  0.009  0.009  0.957  0.009\n",
       "4    0.014  0.014  0.014  0.014  0.932  0.014\n",
       "..     ...    ...    ...    ...    ...    ...\n",
       "939  0.009  0.009  0.009  0.009  0.956  0.009\n",
       "940  0.008  0.008  0.008  0.008  0.958  0.008\n",
       "941  0.008  0.008  0.008  0.008  0.961  0.008\n",
       "942  0.012  0.012  0.012  0.012  0.938  0.012\n",
       "943  0.008  0.008  0.008  0.008  0.961  0.008\n",
       "\n",
       "[944 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_df = pd.DataFrame(lda_df_dict)\n",
    "lda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afc4144c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dominant_topic = [np.argmax(lda_df.values, axis=1)]\n",
    "dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70f168e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANNUNCIATION</td>\n",
       "      <td>LAUREN GROFF</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ONCE REMOVED</td>\n",
       "      <td>ALEXANDER MACLEOD</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LONG DISTANCE</td>\n",
       "      <td>AYSEGUL SAVAS</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WHAT'S THE DEAL, HUMMINGBIRD?</td>\n",
       "      <td>ARTHUR KRYSTAL</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FIREWORKS</td>\n",
       "      <td>GRAHAM SWIFT</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>THE GUARDIANS</td>\n",
       "      <td>JOHN UPDIKE</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>U.F.O. IN KUSHIRO</td>\n",
       "      <td>HARUKI MURAKAMI</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>NOBODY'S BUSINESS</td>\n",
       "      <td>JHUMPA LAHIRI</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>THE SHAWL</td>\n",
       "      <td>LOUISE ERDRICH</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>WHAT IS REMEMBERED</td>\n",
       "      <td>ALICE MUNRO</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>944 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             TITLE             AUTHOR      0      1      2  \\\n",
       "0                     ANNUNCIATION       LAUREN GROFF  0.007  0.007  0.007   \n",
       "1                     ONCE REMOVED  ALEXANDER MACLEOD  0.008  0.008  0.008   \n",
       "2                    LONG DISTANCE      AYSEGUL SAVAS  0.008  0.008  0.008   \n",
       "3    WHAT'S THE DEAL, HUMMINGBIRD?     ARTHUR KRYSTAL  0.009  0.009  0.009   \n",
       "4                        FIREWORKS       GRAHAM SWIFT  0.014  0.014  0.014   \n",
       "..                             ...                ...    ...    ...    ...   \n",
       "939                  THE GUARDIANS        JOHN UPDIKE  0.009  0.009  0.009   \n",
       "940              U.F.O. IN KUSHIRO    HARUKI MURAKAMI  0.008  0.008  0.008   \n",
       "941              NOBODY'S BUSINESS      JHUMPA LAHIRI  0.008  0.008  0.008   \n",
       "942                      THE SHAWL     LOUISE ERDRICH  0.012  0.012  0.012   \n",
       "943             WHAT IS REMEMBERED        ALICE MUNRO  0.008  0.008  0.008   \n",
       "\n",
       "         3      4      5  \n",
       "0    0.007  0.965  0.007  \n",
       "1    0.008  0.960  0.008  \n",
       "2    0.008  0.958  0.008  \n",
       "3    0.009  0.957  0.009  \n",
       "4    0.014  0.932  0.014  \n",
       "..     ...    ...    ...  \n",
       "939  0.009  0.956  0.009  \n",
       "940  0.008  0.958  0.008  \n",
       "941  0.008  0.961  0.008  \n",
       "942  0.012  0.938  0.012  \n",
       "943  0.008  0.961  0.008  \n",
       "\n",
       "[944 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_df.insert(0, 'TITLE', corpus_df.TITLE, allow_duplicates=False)\n",
    "lda_df.insert(1, 'AUTHOR', corpus_df.AUTHOR, allow_duplicates=True)\n",
    "lda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e465ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.9585572)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at sally rooney, unread messages\n",
    "lda.get_document_topics(corpus[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "020913e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.010164557),\n",
       " (1, 0.010164559),\n",
       " (2, 0.01016455),\n",
       " (3, 0.0101645505),\n",
       " (4, 0.94917727),\n",
       " (5, 0.01016456)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at george saunders, semplica girl diaries\n",
    "lda.get_document_topics(corpus[462])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050abad",
   "metadata": {},
   "source": [
    "# CorEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install corextopic\n",
    "#!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8209078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: says, asks, thinks, looks, feels, takes, sees, doesn, goes, tells\n",
      "2: internet, eve, backward, leader, plus, current, ~precisely, ~precious, ~pre, ~praying\n",
      "3: jack, hope, ~relatively, ~released, ~reliable, ~relieved, ~remained, ~reminded, remorse, ~remove\n",
      "4: abandoned, press, pressed, pretend, pretty, prevent, pride, ~priest, printed, privacy\n",
      "5: abandon, problem, professional, ~promises, pronounced, proof, propped, prospect, protection, protested\n"
     ]
    }
   ],
   "source": [
    "from corextopic import corextopic as ct\n",
    "\n",
    "docs = corpus_df.TEXT\n",
    "tf = TfidfVectorizer(stop_words='english', min_df=0.05, max_df=0.60, ngram_range=(1,2))\n",
    "tfidf = tf.fit_transform(docs)\n",
    "vocab = tf.get_feature_names()\n",
    "\n",
    "model = ct.Corex(n_hidden=5, seed=42)\n",
    "model = model.fit(tfidf, words=vocab)\n",
    "\n",
    "topics = model.get_topics()\n",
    "for topic_n,topic in enumerate(topics):\n",
    "    # w: word, mi: mutual information, s: sign\n",
    "    topic = [(w,mi,s) if s > 0 else ('~'+w,mi,s) for w,mi,s in topic]\n",
    "    # Unpack the info about the topic\n",
    "    words,mis,signs = zip(*topic)    \n",
    "    # Print topic\n",
    "    topic_str = str(topic_n+1)+': '+', '.join(words)\n",
    "    print(topic_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96940297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: lift, hut, trail, pass, snow, pole, mountain, valley, slope, ridge\n",
      "2: ~account, ~reading, razor, ray, railing, rage, rag, ~race, rabbit, ~prospect\n",
      "3: act, ~reflection, ~rearview mirror, rate, ~range, rack, punishment, ~punch, ~pump, public\n",
      "4: ~accident, ~prize, ~privacy, ~principle, ~presence, prayer, ~power, ~powder, ~posture, pork\n",
      "5: absence, ~place man, ~pitch, ~pit, pink, ~pin, piece paper, ~pickup, phone, philosophy\n"
     ]
    }
   ],
   "source": [
    "docs = corpus_df.TEXT_NOUNS\n",
    "tf = TfidfVectorizer(stop_words='english', min_df=0.05, max_df=0.60, ngram_range=(1,2))\n",
    "tfidf = tf.fit_transform(docs)\n",
    "vocab = tf.get_feature_names()\n",
    "\n",
    "model = ct.Corex(n_hidden=5, seed=42)\n",
    "model = model.fit(tfidf, words=vocab)\n",
    "\n",
    "topics = model.get_topics()\n",
    "for topic_n,topic in enumerate(topics):\n",
    "    # w: word, mi: mutual information, s: sign\n",
    "    topic = [(w,mi,s) if s > 0 else ('~'+w,mi,s) for w,mi,s in topic]\n",
    "    # Unpack the info about the topic\n",
    "    words,mis,signs = zip(*topic)    \n",
    "    # Print topic\n",
    "    topic_str = str(topic_n+1)+': '+', '.join(words)\n",
    "    print(topic_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb643a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: freezer, pizza, teacher, english, way feel, chest, ~rainy, ~ragged, ~rage, ~range\n",
      "2: register, cash, daddy, drone, mall, battery, birthday, ~plague, ~railing, ~quote\n",
      "3: boat, bridge, arch, skull, bank, river, current, stream, mobile, ~remove\n",
      "4: frog, lick, pond, tongue, mud, transport, fairy, ex, ~represent, ~reservation\n",
      "5: internet, poem, leader, poet, poetry, reader, ~absent, ~prison, private, ~professional\n",
      "6: able, pre, praise, possible, portrait, ~plenty, pleased, ~plaster, plan, ~pipe\n",
      "7: accompany, ~rattle, ~rat, ~rake, ~raise hand, ~radio, punishment, ~pull, pub, proud\n",
      "8: ~accident, ~previous, ~preserve, ~present, ~predict, ~powerful, powder, ~pour glass, ~position, ~polite\n"
     ]
    }
   ],
   "source": [
    "docs = corpus_df.TEXT_NOUNS_VERBS_ADJS\n",
    "tf = TfidfVectorizer(stop_words='english', min_df=0.05, max_df=0.80, ngram_range=(1,2))\n",
    "tfidf = tf.fit_transform(docs)\n",
    "vocab = tf.get_feature_names()\n",
    "\n",
    "anchors = [\n",
    "    [\"age\", \"lifetime\", \"life\", \"death\", \"memory\"],\n",
    "    [\"COVID\", \"pandemic\", \"virus\", \"plague\"],\n",
    "    [\"attraction\", \"desire\", \"sex\", \"romance\", \"girlfriend\", \"boyfriend\", \"wedding\", \"marriage\"],\n",
    "    [\"paycheck\", \"bill\", \"estate\", \"rate\", \"debit\", \"credit\"],\n",
    "    [\"English\", \"immigrant\", \"country\"], [], [], []\n",
    "]\n",
    "\n",
    "anchors = [\n",
    "    [a for a in topic if a in vocab]\n",
    "    for topic in anchors\n",
    "]\n",
    "\n",
    "model = ct.Corex(n_hidden=8, seed=42)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab,\n",
    "    anchors=anchors, # Pass the anchors in here\n",
    "    anchor_strength=4 # Tell the model how much it should rely on the anchors\n",
    ")\n",
    "\n",
    "topics = model.get_topics()\n",
    "for topic_n,topic in enumerate(topics):\n",
    "    # w: word, mi: mutual information, s: sign\n",
    "    topic = [(w,mi,s) if s > 0 else ('~'+w,mi,s) for w,mi,s in topic]\n",
    "    # Unpack the info about the topic\n",
    "    words,mis,signs = zip(*topic)    \n",
    "    # Print topic\n",
    "    topic_str = str(topic_n+1)+': '+', '.join(words)\n",
    "    print(topic_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22fb9218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: death,memory,lifetime,widow,discover,fear,past,presence,form,spirit\n",
      "1: plague,virus,pandemic,scan,freak,vacation,effect,bullshit,garbage,image\n",
      "2: sex,marriage,girlfriend,desire,boyfriend,romance,wedding,kiss,attraction,bathroom\n",
      "3: rate,estate,debt,credit,problem,site,deal,project,plan,dollar\n",
      "4: country,roll,nose,rise,grip,breath,neck,wave,squeeze,ground\n",
      "5: ivy,mouthpiece,trailer,craze,anesthetic,gypsy,hoodie,litre,bacon,irregularity\n"
     ]
    }
   ],
   "source": [
    "#try with countvectorizer\n",
    "from corextopic import corextopic as ct\n",
    "docs = corpus_df.TEXT_NOUNS_VERBS\n",
    "vectorizer = CountVectorizer(max_features=20000,\n",
    "                             stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                             binary=True, max_df=0.60)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(docs)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n",
    "\n",
    "anchors = [\n",
    "    [\"age\", \"lifetime\", \"life\", \"death\", \"memory\", \"die\", \"remember\", \"memorial\", \"widow\"],\n",
    "    [\"COVID\", \"pandemic\", \"virus\", \"plague\"],\n",
    "    [\"attraction\", \"desire\", \"sex\", \"romance\", \"girlfriend\", \"boyfriend\", \"wedding\", \"marriage\", \"kiss\"],\n",
    "    [\"paycheck\", \"bill\", \"estate\", \"rate\", \"debit\", \"credit\", \"pay\", \"debt\"],\n",
    "    [\"English\", \"immigrant\", \"country\", \"immigrate\"]\n",
    "]\n",
    "\n",
    "anchors = [\n",
    "    [a for a in topic if a in words]\n",
    "    for topic in anchors\n",
    "]\n",
    "\n",
    "\n",
    "topic_model = ct.Corex(n_hidden=6, words=words, seed=1)\n",
    "topic_model.fit(doc_word, words=words, docs=docs, anchors=anchors, anchor_strength=4)\n",
    "\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b1cbc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: roll,breath,sky,wave,ground,palm,tooth,rise,sink,drag\n",
      "1: self,office,offer,check,college,problem,result,lack,game,coffee\n",
      "2: childhood,manner,engage,teacher,folder,spoil,script,calendar,overlook,dazzle\n",
      "3: translate,fern,underline,yearn,slit,barrage,shin,wanna,barn,rebellion\n",
      "4: exile,domain,excel,assure,terminate,scandalize,waste,banner,south,deem\n",
      "5: mouthpiece,ivy,pawn,interpretation,bungalow,craze,anesthetic,donate,barbecue,trailer\n"
     ]
    }
   ],
   "source": [
    "#try with countvectorizer, no anchors\n",
    "from corextopic import corextopic as ct\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=20000,\n",
    "                             stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                             max_df=0.60, binary=True)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(docs)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n",
    "\n",
    "topic_model = ct.Corex(n_hidden=6, words=words, seed=1)\n",
    "topic_model.fit(doc_word, words=words, docs=docs)\n",
    "\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8e950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
